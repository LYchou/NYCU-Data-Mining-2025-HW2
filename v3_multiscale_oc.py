"""
v3 Multiscale One-Class Anomaly Detection (KMeans + OCSVM + optional AE)

- Train only on normal images in Dataset/train
- Infer on Dataset/test and output v3_submission.csv with id,prediction
  where id is the numeric filename order (0.png -> id 0)

This script is CPU-friendly and optionally supports a tiny convolutional
autoencoder if PyTorch is available. The AE path is controlled by --ae_enabled.

================================================================================
使用說明 / Usage Guide
================================================================================

【基本使用 - 一次訓練+推論（不含 AE，CPU 友善）】
    python v3_multiscale_oc.py --mode all --ae_enabled false --output_csv v3_submission.csv

【基本使用 - 含 AE（手動指定裝置）】
    python v3_multiscale_oc.py --mode all --ae_enabled true --ae_device cuda --output_csv v3_submission.csv

【分開執行 - 先訓練】
    python v3_multiscale_oc.py --mode train --ae_enabled false

【分開執行 - 再推論（需先執行過 train）】
    python v3_multiscale_oc.py --mode infer --ae_enabled false --output_csv v3_submission.csv

================================================================================
主要參數說明 / Key Parameters
================================================================================

--mode {train,infer,all}
    執行模式：train=僅訓練、infer=僅推論、all=訓練+推論（預設）

--train_dir, --test_dir
    訓練與測試資料夾路徑（預設：Dataset/train, Dataset/test）

--output_csv
    輸出 CSV 檔案路徑（預設：v3_submission.csv）

--workdir
    模型與校準器存放目錄（預設：artifacts/）

--ae_enabled {true,false}
    是否啟用 Autoencoder（預設：false）
    - false：僅使用 KMeans + OCSVM（CPU 友善，約 10-30 分鐘）
    - true：加入 AE（若有 GPU 約 35-75 分鐘；若僅 CPU 會自動退回 CPU 模式）

--ae_device {cpu,cuda}
    AE 使用的裝置（手動指定，不做自動偵測）

--input_w, --input_h
    輸入影像尺寸（預設：256x256）

--patch_sizes
    多尺度 patch 大小（預設：64 128，即兩個尺度）

--patch_stride_ratio
    Patch 滑動步長比例（預設：0.5，即 stride = patch_size * 0.5）

--kmeans_k
    KMeans 群集數（預設：32）

--ocsvm_nu
    OCSVM 異常比例參數（預設：0.1）

--ocsvm_subsample
    OCSVM 訓練子樣本數（預設：16000，加快訓練）

--threshold
    將連續分數轉為 0/1 的閾值（預設：0.5）

--topk_ratio
    每張影像取 top-K% 最可疑 patch 平均（預設：0.05，即前 5%）

--blend_kmeans, --blend_ocsvm, --blend_ae
    各模型的加權係數（預設：kmeans=0.5, ocsvm=0.5；若啟用 AE：kmeans=0.3, ocsvm=0.3, ae=0.4）

================================================================================
輸出說明 / Output
================================================================================

1. 模型產物（存放在 --workdir，預設 artifacts/）：
   - mean_std.npz：訓練集 RGB 平均與標準差
   - pca.joblib：PCA 降維模型
   - kmeans.joblib：KMeans 模型
   - ocsvm.joblib：OCSVM 模型
   - calib_kmeans.json, calib_ocsvm.json：各模型分位數校準參數
   - ae.pt, calib_ae.json（若 --ae_enabled true）：AE 權重與校準參數

2. 提交檔案（--output_csv，預設 v3_submission.csv）：
   - CSV 格式：id,prediction（第一行為標題）
   - id 為 0..N-1（依 Dataset/test 檔名數字排序）
   - prediction 為 0（正常）或 1（異常）

================================================================================
系統需求 / Requirements
================================================================================

基本套件（必裝）：
    numpy, scikit-learn, scikit-image, Pillow, tqdm, joblib

可選套件（僅啟用 AE 時需安裝）：
    torch, torchvision
    - Mac M1：安裝 CPU 版即可
    - Windows + GPU：安裝對應 CUDA 版本可加速

記憶體建議：
    - Mac M1 16GB：可運行（不含 AE 約 10-30 分鐘，含 AE 約 35-75 分鐘）
    - Windows 32GB + RTX 3060 Ti：可完全運行，AE 訓練更快

================================================================================
Author: generated by assistant per user's plan
"""

from __future__ import annotations

import argparse
import json
import math
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import joblib
import numpy as np
from PIL import Image
from tqdm import tqdm

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.svm import OneClassSVM

from skimage.color import rgb2gray
from skimage.feature import local_binary_pattern
from skimage.filters import sobel, laplace, gaussian


# -------------------------------
# Global configuration (centralized defaults)
# -------------------------------

CONFIG = {
    "paths": {
        "train_dir": "Dataset/train",
        "test_dir": "Dataset/test",
        "workdir": "artifacts",
        "output_csv": "v3_submission.csv",
    },
    "image": {"input_w": 256, "input_h": 256},
    "patch": {"sizes": [64, 128], "stride_ratio": 0.5},
    "feature": {"edge_threshold": 0.2, "dog_sigma1": 1.0, "dog_sigma2": 2.5},
    "pca": {"keep_var": 0.95, "max_dim": 64},
    "kmeans": {"k": 32, "n_init": 10},
    "ocsvm": {"nu": 0.1, "gamma_mode": "1_over_d", "subsample": 16000},
    "calibrate": {"q_low": 0.05, "q_high": 0.95},
    "aggregate": {
        "topk_ratio": 0.05,
        "blend_kmeans": 0.5,
        "blend_ocsvm": 0.5,
        "blend_ae": 0.4,
        "threshold": 0.5,
    },
    "ae": {
        "enabled": False,
        "latent_dim": 48,
        "epochs": 20,
        "batch_size": 256,
        "lr": 1.5e-3,
        "device": "cpu",  # no auto-detect; set to 'cpu' or 'cuda'
    },
    "seed": 42,
}

# -------------------------------
# Utilities
# -------------------------------


def set_seed(seed: int) -> None:
    np.random.seed(seed)


def list_png_numeric(dir_path: str) -> List[str]:
    p = Path(dir_path)
    if not p.exists():
        raise FileNotFoundError(f"Directory not found: {dir_path}")
    paths = [str(x) for x in p.glob("*.png")]
    if not paths:
        raise RuntimeError(f"No PNG images in {dir_path}")

    def key_fn(s: str) -> Tuple[int, str]:
        stem = Path(s).stem
        try:
            return (int(stem), "")
        except ValueError:
            return (sys.maxsize, stem)

    paths.sort(key=key_fn)
    return paths


def load_image_rgb(path: str, size_hw: Tuple[int, int]) -> np.ndarray:
    with Image.open(path) as img:
        img = img.convert("RGB")
        img = img.resize((size_hw[1], size_hw[0]), Image.BILINEAR)
        arr = np.asarray(img, dtype=np.float32) / 255.0
    return arr  # H,W,3 in [0,1]


def online_mean_std(
    image_paths: Sequence[str], size_hw: Tuple[int, int]
) -> Tuple[np.ndarray, np.ndarray]:
    """Compute global mean/std over the whole training set (RGB channels).

    Uses stable online accumulation to avoid keeping all pixels in memory.
    Returns (mean, std) with shape (3,).
    """
    count = 0
    sum_c = np.zeros(3, dtype=np.float64)
    sumsq_c = np.zeros(3, dtype=np.float64)
    for p in tqdm(image_paths, desc="Compute train mean/std"):
        img = load_image_rgb(p, size_hw)
        pix = img.reshape(-1, 3).astype(np.float64)
        count += pix.shape[0]
        sum_c += pix.sum(axis=0)
        sumsq_c += (pix * pix).sum(axis=0)
    mean = sum_c / max(count, 1)
    var = sumsq_c / max(count, 1) - mean * mean
    var = np.maximum(var, 1e-12)
    std = np.sqrt(var)
    return mean.astype(np.float32), (std + 1e-6).astype(np.float32)


def standardize_rgb(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
    return (arr - mean[None, None, :]) / std[None, None, :]


# -------------------------------
# Patch extraction
# -------------------------------


def sliding_windows(img: np.ndarray, patch: int, stride: int) -> Iterable[np.ndarray]:
    H, W, _ = img.shape
    if H < patch or W < patch:
        return []
    for y in range(0, H - patch + 1, stride):
        for x in range(0, W - patch + 1, stride):
            yield img[y : y + patch, x : x + patch, :]


def make_patches(img: np.ndarray, patch_sizes: Sequence[int], stride_ratio: float) -> List[np.ndarray]:
    patches: List[np.ndarray] = []
    for p in patch_sizes:
        stride = max(1, int(p * stride_ratio))
        patches.extend(list(sliding_windows(img, p, stride)))
    return patches


# -------------------------------
# Feature extraction
# -------------------------------


@dataclass
class FeatureConfig:
    use_lbp: bool = True
    use_edges: bool = True
    use_dog: bool = True
    use_fft: bool = True
    lbp_P: int = 8
    lbp_R: float = 1.0
    edge_threshold: float = 0.2
    dog_sigma1: float = 1.0
    dog_sigma2: float = 2.5
    fft_bins: Tuple[float, float, float, float] = (0.0, 0.2, 0.5, 1.0)


def feats_from_patch(rgb_patch: np.ndarray, cfg: FeatureConfig) -> np.ndarray:
    gray = rgb2gray(rgb_patch).astype(np.float32)
    vec_parts: List[np.ndarray] = []

    if cfg.use_lbp:
        lbp = local_binary_pattern(gray, P=cfg.lbp_P, R=cfg.lbp_R, method="uniform")
        bins = np.arange(0, cfg.lbp_P + 3)
        hist, _ = np.histogram(lbp, bins=bins, range=(0, cfg.lbp_P + 2), density=True)
        vec_parts.append(hist.astype(np.float32))

    if cfg.use_edges:
        ed1 = sobel(gray)
        ed2 = laplace(gray)
        thr = cfg.edge_threshold
        edge_stats = np.array(
            [
                float(ed1.mean()),
                float(ed1.std()),
                float((ed1 > thr).mean()),
                float(ed2.mean()),
                float(ed2.std()),
                float((ed2 > thr).mean()),
            ],
            dtype=np.float32,
        )
        vec_parts.append(edge_stats)

    if cfg.use_dog:
        g1 = gaussian(gray, sigma=cfg.dog_sigma1)
        g2 = gaussian(gray, sigma=cfg.dog_sigma2)
        dog = np.abs(g1 - g2)
        dog_stats = np.array(
            [float(dog.mean()), float(dog.std()), float((dog > 0.1).mean())],
            dtype=np.float32,
        )
        vec_parts.append(dog_stats)

    if cfg.use_fft:
        # real fft for speed; magnitude normalization
        F = np.abs(np.fft.rfft2(gray))
        F = F / (F.sum() + 1e-8)
        Hf, Wf = F.shape
        yy, xx = np.mgrid[0:Hf, 0:Wf]
        r = np.sqrt((yy / max(Hf - 1, 1)) ** 2 + (xx / max(Wf - 1, 1)) ** 2)
        bins = list(cfg.fft_bins)
        bucket: List[float] = []
        for i in range(len(bins) - 1):
            m = (r >= bins[i]) & (r < bins[i + 1])
            bucket.append(float(F[m].sum()))
        vec_parts.append(np.asarray(bucket, dtype=np.float32))

    return np.concatenate(vec_parts, axis=0)


def extract_features_from_patches(patches: Sequence[np.ndarray], cfg: FeatureConfig) -> np.ndarray:
    if len(patches) == 0:
        return np.empty((0, 0), dtype=np.float32)
    feats: List[np.ndarray] = []
    for p in patches:
        feats.append(feats_from_patch(p, cfg))
    X = np.stack(feats, axis=0).astype(np.float32)
    return X


# -------------------------------
# PCA reducer
# -------------------------------


def fit_pca_dynamic(X: np.ndarray, keep_var: float, max_dim: int, random_state: int) -> PCA:
    max_dim_eff = min(max_dim, X.shape[1])
    pca = PCA(n_components=max_dim_eff, random_state=random_state)
    pca.fit(X)
    cum = np.cumsum(pca.explained_variance_ratio_)
    idx = int(np.searchsorted(cum, keep_var) + 1)
    n_components = min(max_dim_eff, max(1, idx))
    if n_components != pca.n_components_:
        pca = PCA(n_components=n_components, random_state=random_state)
        pca.fit(X)
    return pca


# -------------------------------
# One-class models
# -------------------------------


def kmeans_distance_scores(X: np.ndarray, centers: np.ndarray) -> np.ndarray:
    # ||x - c||^2 = ||x||^2 + ||c||^2 - 2 x.c
    x2 = np.sum(X * X, axis=1, keepdims=True)
    c2 = np.sum(centers * centers, axis=1)
    cross = X @ centers.T
    d2 = x2 + c2[None, :] - 2.0 * cross
    d2 = np.maximum(d2, 0.0)
    d = np.sqrt(np.min(d2, axis=1))
    return d.astype(np.float32)


def ocsvm_scores(model: OneClassSVM, X: np.ndarray) -> np.ndarray:
    # decision_function: higher -> more normal. We want anomaly score: negative.
    return (-model.decision_function(X).reshape(-1)).astype(np.float32)


# -------------------------------
# Calibration and aggregation
# -------------------------------


def fit_quantile_calibrator(scores: np.ndarray, q_low: float, q_high: float) -> Tuple[float, float]:
    lo = float(np.quantile(scores, q_low))
    hi = float(np.quantile(scores, q_high))
    if not math.isfinite(lo):
        lo = float(np.min(scores))
    if not math.isfinite(hi):
        hi = float(np.max(scores) + 1e-6)
    if hi <= lo:
        hi = lo + 1e-6
    return lo, hi


def apply_calibrator(scores: np.ndarray, lo: float, hi: float) -> np.ndarray:
    z = (scores - lo) / (hi - lo)
    return np.clip(z, 0.0, 1.0)


def topk_mean(values: np.ndarray, ratio: float) -> float:
    if values.size == 0:
        return 0.0
    n = max(1, int(values.size * ratio))
    return float(np.sort(values)[-n:].mean())


def aggregate_image_scores(
    per_patch_scores: Dict[str, Optional[np.ndarray]], topk_ratio: float, blend: Dict[str, float]
) -> float:
    pooled: Dict[str, float] = {}
    for name, arr in per_patch_scores.items():
        if arr is None or arr.size == 0:
            continue
        pooled[name] = topk_mean(arr, topk_ratio)
    num = 0.0
    den = 0.0
    for name, w in blend.items():
        if name in pooled:
            num += w * pooled[name]
            den += w
    return num / max(den, 1e-8)


# -------------------------------
# Optional Autoencoder (PyTorch)
# -------------------------------


class TinyAE:
    """A tiny Conv-AE for 64x64 grayscale patches.

    Implemented lazily (imports torch only if used).
    """

    def __init__(self, latent_dim: int = 48, device: str = "cpu") -> None:
        import torch
        import torch.nn as nn

        class Net(nn.Module):
            def __init__(self, z: int):
                super().__init__()
                self.enc = nn.Sequential(
                    nn.Conv2d(1, 16, 3, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(16, 32, 3, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(32, 64, 3, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                )  # 64->8
                self.enc_fc = nn.Linear(64 * 8 * 8, z)
                self.dec_fc = nn.Linear(z, 64 * 8 * 8)
                self.dec = nn.Sequential(
                    nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),
                    nn.Sigmoid(),
                )

            def forward(self, x):
                h = self.enc(x)
                h = h.flatten(1)
                z = self.enc_fc(h)
                h2 = self.dec_fc(z).view(x.size(0), 64, 8, 8)
                out = self.dec(h2)
                return out

        self.device = device
        self.net = Net(latent_dim).to(self.device)

    def train_fit(
        self,
        patches64_gray: np.ndarray,
        epochs: int = 20,
        batch_size: int = 256,
        lr: float = 1.5e-3,
    ) -> None:
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset

        x = torch.from_numpy(patches64_gray).float().unsqueeze(1)  # N,1,64,64
        ds = TensorDataset(x, x)
        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)
        opt = torch.optim.Adam(self.net.parameters(), lr=lr)
        crit = nn.MSELoss(reduction="mean")
        self.net.train()
        for _ in tqdm(range(epochs), desc="Train AE"):
            for xb, yb in dl:
                xb = xb.to(self.device)
                yb = yb.to(self.device)
                opt.zero_grad(set_to_none=True)
                out = self.net(xb)
                loss = crit(out, yb)
                loss.backward()
                opt.step()

    def score(self, patches64_gray: np.ndarray) -> np.ndarray:
        import torch
        import torch.nn.functional as F

        self.net.eval()
        with torch.no_grad():
            x = torch.from_numpy(patches64_gray).float().unsqueeze(1).to(self.device)
            out = self.net(x)
            mse = F.mse_loss(out, x, reduction="none")
            # Reduce over C,H,W
            mse = mse.view(mse.size(0), -1).mean(dim=1).cpu().numpy().astype(np.float32)
        return mse

    def save(self, path: str) -> None:
        import torch

        Path(path).parent.mkdir(parents=True, exist_ok=True)
        torch.save(self.net.state_dict(), path)

    def load(self, path: str) -> None:
        import torch

        self.net.load_state_dict(torch.load(path, map_location=self.device))


# -------------------------------
# Train / Infer pipeline
# -------------------------------


def train_pipeline(args: argparse.Namespace) -> None:
    set_seed(args.seed)

    train_paths = list_png_numeric(args.train_dir)
    ih, iw = CONFIG["image"]["input_h"], CONFIG["image"]["input_w"]
    mean, std = online_mean_std(train_paths, (ih, iw))

    # Extract features from multi-scale patches
    feat_cfg = FeatureConfig(
        use_lbp=True,
        use_edges=True,
        use_dog=True,
        use_fft=True,
        lbp_P=8,
        lbp_R=1.0,
        edge_threshold=CONFIG["feature"]["edge_threshold"],
        dog_sigma1=CONFIG["feature"]["dog_sigma1"],
        dog_sigma2=CONFIG["feature"]["dog_sigma2"],
        fft_bins=(0.0, 0.2, 0.5, 1.0),
    )

    X_list: List[np.ndarray] = []
    patches64_for_ae: List[np.ndarray] = []
    for p in tqdm(train_paths, desc="Extract train features"):
        img = load_image_rgb(p, (ih, iw))
        img = standardize_rgb(img, mean, std)
        patches = make_patches(img, CONFIG["patch"]["sizes"], CONFIG["patch"]["stride_ratio"])
        Xp = extract_features_from_patches(patches, feat_cfg)
        X_list.append(Xp)
        if args.ae_enabled:
            # collect 64x64 grayscale patches for AE only
            for patch_sz in CONFIG["patch"]["sizes"]:
                if patch_sz == 64:
                    stride = max(1, int(patch_sz * CONFIG["patch"]["stride_ratio"]))
                    for win in sliding_windows(img, patch_sz, stride):
                        g = rgb2gray(win).astype(np.float32)
                        if g.shape[0] == 64 and g.shape[1] == 64:
                            patches64_for_ae.append(g)

    X = np.concatenate(X_list, axis=0) if len(X_list) > 0 else np.empty((0, 0), dtype=np.float32)

    # PCA
    pca = fit_pca_dynamic(X, CONFIG["pca"]["keep_var"], CONFIG["pca"]["max_dim"], args.seed)
    X_pca = pca.transform(X)

    # KMeans
    kmeans = KMeans(n_clusters=CONFIG["kmeans"]["k"], n_init=CONFIG["kmeans"]["n_init"], random_state=args.seed)
    kmeans.fit(X_pca)
    km_scores_train = kmeans_distance_scores(X_pca, kmeans.cluster_centers_)
    km_lo, km_hi = fit_quantile_calibrator(km_scores_train, CONFIG["calibrate"]["q_low"], CONFIG["calibrate"]["q_high"])

    # OCSVM on a subsample for stability and speed
    ocsvm_sub_n = min(CONFIG["ocsvm"]["subsample"], X_pca.shape[0])
    if ocsvm_sub_n <= 0:
        raise RuntimeError("Invalid OCSVM subsample size")
    idx = np.random.choice(X_pca.shape[0], size=ocsvm_sub_n, replace=False)
    X_sub = X_pca[idx]
    gamma = 1.0 / max(1, X_pca.shape[1]) if CONFIG["ocsvm"]["gamma_mode"] == "1_over_d" else "scale"
    ocsvm = OneClassSVM(kernel="rbf", nu=CONFIG["ocsvm"]["nu"], gamma=gamma)
    ocsvm.fit(X_sub)
    svm_scores_train = ocsvm_scores(ocsvm, X_pca)
    svm_lo, svm_hi = fit_quantile_calibrator(svm_scores_train, CONFIG["calibrate"]["q_low"], CONFIG["calibrate"]["q_high"])

    # Optional AE
    ae_state = None
    ae_lo = ae_hi = None
    if args.ae_enabled:
        try:
            import torch  # noqa: F401
            device = args.ae_device  # manual selection: 'cpu' or 'cuda'
            patches64 = np.stack(patches64_for_ae, axis=0) if len(patches64_for_ae) else np.empty((0, 64, 64), dtype=np.float32)
            ae = TinyAE(latent_dim=CONFIG["ae"]["latent_dim"], device=device)
            if patches64.shape[0] > 0:
                ae.train_fit(patches64, epochs=CONFIG["ae"]["epochs"], batch_size=CONFIG["ae"]["batch_size"], lr=CONFIG["ae"]["lr"])
                ae_scores = ae.score(patches64)
                ae_lo, ae_hi = fit_quantile_calibrator(ae_scores, CONFIG["calibrate"]["q_low"], CONFIG["calibrate"]["q_high"])
                # save AE weights below
                ae_state = ae
            else:
                print("[AE] No 64x64 patches collected; skipping AE calibration.")
        except Exception as e:
            print(f"[AE] Torch not available or AE failed ({e}); AE will be disabled for inference.")
            args.ae_enabled = False

    # Save artifacts
    work = Path(args.workdir)
    work.mkdir(parents=True, exist_ok=True)
    np.savez_compressed(work / "mean_std.npz", mean=mean, std=std)
    joblib.dump(pca, work / "pca.joblib")
    joblib.dump(kmeans, work / "kmeans.joblib")
    joblib.dump(ocsvm, work / "ocsvm.joblib")
    with open(work / "calib_kmeans.json", "w", encoding="utf-8") as f:
        json.dump({"lo": km_lo, "hi": km_hi}, f)
    with open(work / "calib_ocsvm.json", "w", encoding="utf-8") as f:
        json.dump({"lo": svm_lo, "hi": svm_hi}, f)
    if args.ae_enabled and ae_state is not None and ae_lo is not None and ae_hi is not None:
        # Save AE
        try:
            ae_state.save(str(work / "ae.pt"))
            with open(work / "calib_ae.json", "w", encoding="utf-8") as f:
                json.dump({"lo": float(ae_lo), "hi": float(ae_hi)}, f)
        except Exception as e:
            print(f"[AE] Failed to save AE ({e}).")

    print(f"Artifacts saved to {work}")


def ensure_artifacts_exist(workdir: Path, need_ae: bool) -> None:
    required = ["mean_std.npz", "pca.joblib", "kmeans.joblib", "ocsvm.joblib"]
    missing = [f for f in required if not (workdir / f).exists()]
    if missing:
        raise RuntimeError(f"Artifacts missing {missing}. 請先執行 --mode train")
    if need_ae and not (workdir / "ae.pt").exists():
        raise RuntimeError("AE 權重不存在（artifacts/ae.pt）。請先以 --ae_enabled true 訓練，或推論時關閉 AE。")


def infer_pipeline(args: argparse.Namespace) -> None:
    set_seed(args.seed)

    work = Path(args.workdir)
    ensure_artifacts_exist(work, need_ae=args.ae_enabled)
    # Load artifacts
    mean_std = np.load(work / "mean_std.npz")
    mean = mean_std["mean"].astype(np.float32)
    std = mean_std["std"].astype(np.float32)
    pca: PCA = joblib.load(work / "pca.joblib")
    kmeans: KMeans = joblib.load(work / "kmeans.joblib")
    ocsvm: OneClassSVM = joblib.load(work / "ocsvm.joblib")
    with open(work / "calib_kmeans.json", "r", encoding="utf-8") as f:
        km_calib = json.load(f)
    with open(work / "calib_ocsvm.json", "r", encoding="utf-8") as f:
        svm_calib = json.load(f)

    ae = None
    ae_calib = None
    if args.ae_enabled and (work / "ae.pt").exists() and (work / "calib_ae.json").exists():
        try:
            import torch  # noqa: F401

            device = args.ae_device  # manual: 'cpu' or 'cuda'
            ae = TinyAE(latent_dim=CONFIG["ae"]["latent_dim"], device=device)
            ae.load(str(work / "ae.pt"))
            with open(work / "calib_ae.json", "r", encoding="utf-8") as f:
                ae_calib = json.load(f)
        except Exception as e:
            print(f"[AE] Unable to load AE ({e}); AE disabled.")
            ae = None
            ae_calib = None

    test_paths = list_png_numeric(args.test_dir)

    feat_cfg = FeatureConfig(
        use_lbp=True,
        use_edges=True,
        use_dog=True,
        use_fft=True,
        lbp_P=8,
        lbp_R=1.0,
        edge_threshold=CONFIG["feature"]["edge_threshold"],
        dog_sigma1=CONFIG["feature"]["dog_sigma1"],
        dog_sigma2=CONFIG["feature"]["dog_sigma2"],
        fft_bins=(0.0, 0.2, 0.5, 1.0),
    )

    image_scores: List[float] = []
    ih, iw = CONFIG["image"]["input_h"], CONFIG["image"]["input_w"]
    for p in tqdm(test_paths, desc="Infer test"):
        img = load_image_rgb(p, (ih, iw))
        img = standardize_rgb(img, mean, std)
        patches = make_patches(img, CONFIG["patch"]["sizes"], CONFIG["patch"]["stride_ratio"])
        X = extract_features_from_patches(patches, feat_cfg)
        X_pca = pca.transform(X)
        km_scores = kmeans_distance_scores(X_pca, kmeans.cluster_centers_)
        km_z = apply_calibrator(km_scores, float(km_calib["lo"]), float(km_calib["hi"]))
        svm_scores = ocsvm_scores(ocsvm, X_pca)
        svm_z = apply_calibrator(svm_scores, float(svm_calib["lo"]), float(svm_calib["hi"]))

        ae_z = None
        if ae is not None and ae_calib is not None:
            # compute AE scores on 64x64 patches only
            patches64: List[np.ndarray] = []
            for patch_sz in CONFIG["patch"]["sizes"]:
                if patch_sz == 64:
                    stride = max(1, int(patch_sz * CONFIG["patch"]["stride_ratio"]))
                    for win in sliding_windows(img, patch_sz, stride):
                        g = rgb2gray(win).astype(np.float32)
                        if g.shape[0] == 64 and g.shape[1] == 64:
                            patches64.append(g)
            if len(patches64) > 0:
                arr = np.stack(patches64, axis=0)
                ae_scores = ae.score(arr)
                ae_z = apply_calibrator(ae_scores, float(ae_calib["lo"]), float(ae_calib["hi"]))

        blend = {}
        if args.ae_enabled and (ae is not None) and (ae_z is not None):
            blend = {"kmeans": CONFIG["aggregate"]["blend_kmeans"], "ocsvm": CONFIG["aggregate"]["blend_ocsvm"], "ae": CONFIG["aggregate"]["blend_ae"]}
        else:
            blend = {"kmeans": CONFIG["aggregate"]["blend_kmeans"], "ocsvm": CONFIG["aggregate"]["blend_ocsvm"]}

        img_score = aggregate_image_scores({"kmeans": km_z, "ocsvm": svm_z, "ae": ae_z}, CONFIG["aggregate"]["topk_ratio"], blend)
        image_scores.append(img_score)

    # Convert to 0/1 by threshold
    preds = (np.asarray(image_scores) >= CONFIG["aggregate"]["threshold"]).astype(int)

    # Write submission CSV (id,prediction) with id 0..N-1 in sorted order
    out_csv = Path(args.output_csv)
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    with open(out_csv, "w", encoding="utf-8") as f:
        f.write("id,prediction\n")
        for idx, y in enumerate(preds.tolist()):
            f.write(f"{idx},{y}\n")
    print(f"Wrote submission to {out_csv}")


# -------------------------------
# CLI
# -------------------------------


def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="v3 Multiscale OC Anomaly Detection")
    p.add_argument("--mode", type=str, default="all", choices=["train", "infer", "all"])
    # Keep only essential overrides; others come from CONFIG
    p.add_argument("--ae_enabled", type=lambda x: str(x).lower() in {"1", "true", "yes"}, default=CONFIG["ae"]["enabled"])
    p.add_argument("--ae_device", type=str, choices=["cpu", "cuda"], default=CONFIG["ae"]["device"])  # manual select
    p.add_argument("--output_csv", type=str, default=str(Path(CONFIG["paths"]["output_csv"]).resolve()))
    p.add_argument("--train_dir", type=str, default=str(Path(CONFIG["paths"]["train_dir"]).resolve()))
    p.add_argument("--test_dir", type=str, default=str(Path(CONFIG["paths"]["test_dir"]).resolve()))
    p.add_argument("--workdir", type=str, default=str(Path(CONFIG["paths"]["workdir"]).resolve()))
    p.add_argument("--seed", type=int, default=CONFIG["seed"])
    return p


def main() -> None:
    args = build_arg_parser().parse_args()

    if args.mode in ("train", "all"):
        train_pipeline(args)
    if args.mode in ("infer", "all"):
        infer_pipeline(args)


if __name__ == "__main__":
    main()


